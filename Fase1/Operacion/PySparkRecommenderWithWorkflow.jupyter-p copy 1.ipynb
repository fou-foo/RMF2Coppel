{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Workflow automation for a recommender engine using ALS Model in PySpark\n### This notebook demonstrates the automation of a product recommendation engine given we have user-item interaction data in terms of the frequency of purchase for each unique user-item pair\n### The automation is implemented by deploying the notebook as a job which reads the input data from a remote database, trains and runs the model and writes the output back to the designated database\n"}, {"metadata": {}, "cell_type": "markdown", "source": "## Importing the libraries and starting the Spark Session"}, {"metadata": {}, "cell_type": "code", "source": "#168de5db-7967-4549-8b28-2ae60765f842\n#import os\n#print ( os.environ['KERNEL_ID'])\n", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200924123619-0000\nKERNEL_ID = e44a0d31-1eca-4d86-b860-cf00d23243c0\ne44a0d31-1eca-4d86-b860-cf00d23243c0\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "#System.getenv(\"KERNEL_ID\")", "execution_count": 2, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'System' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-2-8962b74e4ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KERNEL_ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'System' is not defined"]}]}, {"metadata": {}, "cell_type": "code", "source": "### Load packages \nimport time\nt0 = time.time()\nimport pyspark.sql.functions as sql_func\nfrom pyspark.sql.types import *\nfrom pyspark.ml.recommendation import ALS, ALSModel # factorizacion de matrices\nfrom pyspark.context import SparkContext \nfrom pyspark.sql.session import SparkSession\nfrom pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\nfrom pyspark.ml.evaluation import RegressionEvaluator\nimport jaydebeapi, pandas as pd", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200924065743-0002\nKERNEL_ID = c5f13348-c8c7-440f-bcf2-ec6529f2f966\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "### set up spark session and context \nsc = SparkContext.getOrCreate()\nspark = SparkSession(sc)", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Remote connection to the database ( Any supported database like DB2, Netezza, PDA )\n### To enable the connection, add your database under the Data Sources tab in your project. You would need information about your database like JDBC URL, type, username/password\n\n### Adding the asset (for example- the dataset which has the user-item interaction information) from the remote database after setting up the connection"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "%%time\n# @hidden_cell\n# This connection object is used to access your data and contains your credentials.\n# You might want to remove those credentials before you share your notebook.\n\n#PARA LEER REGISTROS DE PDA CON ALTO VOLUMEN (Ej.: 300 MILLONES ) USAR SPARK SQL\n\nfrom project_lib import Project\nproject = Project.access()\nPDA_Analytics_credentials = project.get_connection(name=\"PDA_Analytics\")\n\nfrom pyspark.sql import SparkSession\nsparkSession = SparkSession(spark).builder.getOrCreate()\n\nquery = '''\nSELECT CLIENTECODIGO as ID_CTE, CAST( ADCLAFAM as integer) as ID_CLAS1, \n\t\tFRECUENCIA as FREQUENCY  \n\tFROM(\tSELECT *, TRIM(TO_CHAR(FAMILIA,'000')) AS FAM, TRIM(TO_CHAR(CLASE,'00')) AS CLAS, \n\t                  TRIM(TO_CHAR(AREA,'0')) AS AREA, (AREA||DEPARTAMENTO||CLAS||FAM) AS ADCLAFAM \n\t\t\t\tFROM(    SELECT  CLIENTECODIGO, CLASE, FAMILIA, DEPARTAMENTO, \n\t\t\t\t                 SUM(CASE WHEN CLASE>'0' THEN 1 ELSE 0 END) AS FRECUENCIA,\n\t\t\t\t                 MAX(CASE WHEN CARTERA='Ropa' THEN 1  \n\t\t\t\t\t\t\t\t          WHEN CARTERA='Muebles' THEN 2\n\t\t\t\t\t\t\t\t\t\t  WHEN CARTERA='Prestamos' THEN 3 ELSE 0 END) AS AREA\n\t\t\t\t\t\t\tFROM( SELECT *, CASE WHEN CLASE>'0' THEN 1 ELSE 0 END AS T_CLASE, \n\t\t\t\t\t\t\t               CASE WHEN FAMILIA>'0' THEN 1 ELSE 0 END AS T_FAMILIA\n\t\t\t\t\t\t\t\t\tFROM DIRECCIONRIESGOS.ADMIN.TRANSACCIONESCARTERAS \n\t\t\t\t\t\t\t\t\twhere FECHACORTE between '2017-01-31' and '2019-12-31' \n\t\t\t\t\t\t\t\t\tand CLIENTECODIGO not in (9001,9000) AND CLIENTECODIGO >10000) E \n\t\t\t\tGROUP BY CLIENTECODIGO, CLASE, FAMILIA, DEPARTAMENTO \n\t\t\t\tORDER BY CLIENTECODIGO) T \n\tWHERE CLASE NOT IN (-99)) P \n\tORDER BY CLIENTECODIGO,ADCLAFAM --limit 30000000\n'''\n\ndbTableOrQuery = \"(\" + query + \") TBL\"\n\nt1 = time.time()\n\nspark_df = sparkSession.read.format('jdbc') \\\n    .option('url', 'jdbc:netezza://{}:{}/{}'.format(PDA_Analytics_credentials['host'],PDA_Analytics_credentials['port'],PDA_Analytics_credentials['database'])) \\\n    .option('dbtable', dbTableOrQuery) \\\n    .option(\"numPartitions\", 18) \\\n    .option(\"lowerBound\", 1) \\\n    .option(\"upperBound\", 1000000) \\\n    .option(\"partitionColumn\", \"ID_CTE\") \\\n    .option('user', PDA_Analytics_credentials['username']) \\\n    .option('password', PDA_Analytics_credentials['password']).load()\n\nspark_df.show(5)\n\n\nt2 = time.time()\nseconds_get_data = t2 - t1 \nprint(str(seconds_get_data / 60 ) + ' minutos en transferir tabla PDA - USANDO SPARK SQL')\n\n\n# PARA 300mil registros 0.23564455509185792 minutos en transferir tabla PDA - USANDO SPARK SQL - Usando 8 Executors 2VCPU 8GBRAM\n# PARA 3MILLONES registros 2.376683322588603 minutos en transferir tabla PDA - USANDO SPARK SQL - Usando 8 Executors 2VCPU 8GBRAM\n# PARA 10MILLONES registros 2.254576830069224 minutos en transferir tabla PDA - USANDO SPARK SQL\n# PARA 30MILLONES registros 2.587259344259898 minutos en transferir tabla PDA - USANDO SPARK SQL\n# PARA 50 MILLONES registros 2.624022964636485 minutos en transferir tabla PDA - USANDO SPARK SQL\n# PARA 60 MILLONES registros 3.0992782632509868 minutos en transferir tabla PDA - USANDO SPARK SQL\n# PARA 70 MILLONES registros 7.071753338972727 minutos en transferir tabla PDA - USANDO SPARK SQL\n# PARA 70 MILLONES registros parallel 2.1106858770052592 minutos en transferir tabla PDA - USANDO SPARK SQL\n# PARA 100 MILLONES registros parallel 2.2880197008450827 minutos en transferir tabla PDA - USANDO SPARK SQL\n# PARA TODA EL QUERY (318MILLONES) registros parallel 0.5605056325594584 minutos en transferir tabla PDA - USANDO SPARK SQL", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "+------+--------+---------+\n|ID_CTE|ID_CLAS1|FREQUENCY|\n+------+--------+---------+\n| 10002| 1313064|        1|\n| 10002| 1314064|        3|\n| 10002| 1862008|        4|\n| 10002| 1867048|        2|\n| 10002| 2210070|        1|\n+------+--------+---------+\nonly showing top 5 rows\n\n2.335151453812917 minutos en transferir tabla PDA - USANDO SPARK SQL\nCPU times: user 231 ms, sys: 73.1 ms, total: 304 ms\nWall time: 2min 20s\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "print(spark_df.count())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#SKIP THIS CODE\n# Load JDBC data to Spark dataframe\n# ESTE ES SOLAMENTE CODIGO DE EJEMPLO PARA HACER UN QUERY A UNA CONEXION DE BD CON SPARK SQL\ndbTableOrQuery = '\"' + (dataSet['schema'] + '\".\"' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table'] + '\"'\nif (dataSet['query']):\n    dbTableOrQuery = \"(\" + dataSet['query'] + \") TBL\"\nfinal_stat = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\", dbTableOrQuery).option(\"user\",dataSource['user']).option(\"password\",dataSource['password']).load()\nfinal_stat.show(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#SKIP THIS CODE\n# Access to Porject's conexion\n# and get the data \n# NO RECOMENDABLE PARA DATASETS DE GRAN VOLUMEN USAR PANDAS\n\nfrom project_lib import Project\nproject = Project.access()\nPDA_COPPEL_2020_credentials = project.get_connection(name=\"PDA_Analytics\")\n\nPDA_COPPEL_2020_connection = jaydebeapi.connect('org.netezza.Driver',\n    '{}://{}:{}/{}'.format('jdbc:netezza',\n    PDA_COPPEL_2020_credentials['host'],\n    PDA_COPPEL_2020_credentials['port'],\n    PDA_COPPEL_2020_credentials['database']),\n    [PDA_COPPEL_2020_credentials['username'],\n    PDA_COPPEL_2020_credentials['password']])\n\nquery = '''\nSELECT CLIENTECODIGO as ID_CTE, CAST( ADCLAFAM as integer) as ID_CLAS1, \n\t\tFRECUENCIA as FREQUENCY  \n\tFROM(\tSELECT *, TRIM(TO_CHAR(FAMILIA,'000')) AS FAM, TRIM(TO_CHAR(CLASE,'00')) AS CLAS, \n\t                  TRIM(TO_CHAR(AREA,'0')) AS AREA, (AREA||DEPARTAMENTO||CLAS||FAM) AS ADCLAFAM \n\t\t\t\tFROM(    SELECT  CLIENTECODIGO, CLASE, FAMILIA, DEPARTAMENTO, \n\t\t\t\t                 SUM(CASE WHEN CLASE>'0' THEN 1 ELSE 0 END) AS FRECUENCIA,\n\t\t\t\t                 MAX(CASE WHEN CARTERA='Ropa' THEN 1  \n\t\t\t\t\t\t\t\t          WHEN CARTERA='Muebles' THEN 2\n\t\t\t\t\t\t\t\t\t\t  WHEN CARTERA='Prestamos' THEN 3 ELSE 0 END) AS AREA\n\t\t\t\t\t\t\tFROM( SELECT *, CASE WHEN CLASE>'0' THEN 1 ELSE 0 END AS T_CLASE, \n\t\t\t\t\t\t\t               CASE WHEN FAMILIA>'0' THEN 1 ELSE 0 END AS T_FAMILIA\n\t\t\t\t\t\t\t\t\tFROM DIRECCIONRIESGOS.ADMIN.TRANSACCIONESCARTERAS \n\t\t\t\t\t\t\t\t\twhere FECHACORTE between '2017-01-31' and '2019-12-31' \n\t\t\t\t\t\t\t\t\tand CLIENTECODIGO not in (9001,9000) AND CLIENTECODIGO >10000) E \n\t\t\t\tGROUP BY CLIENTECODIGO, CLASE, FAMILIA, DEPARTAMENTO \n\t\t\t\tORDER BY CLIENTECODIGO) T \n\tWHERE CLASE NOT IN (-99)) P \n\tORDER BY CLIENTECODIGO,ADCLAFAM \n'''\n# 1% ~ 40 minutos \n# 0.1% ~ 6 minutos \n# medimos el tiempo en transferir los datos \nt1 = time.time()\ndata_df_1 = pd.read_sql(query, con=PDA_COPPEL_2020_connection)\nt2 = time.time()\nseconds_get_data = t2 - t1 \nprint(str(seconds_get_data / 60 ) + ' minutos en transferir tabla')\ndata_df_1.head(10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#SKIP THIS CODE\nprint(data_df_1.dtypes) # #  validamos el tipo de dato y numero de registros\nprint(data_df_1.shape)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#SKIP THIS CODE\nspark_df = spark.createDataFrame(data_df_1) # distribuimos el pandas dataframe a spark \n# hacer el get desde spark ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# spark_df.show() #  no es necesario verlo ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Preparing data for the model"}, {"metadata": {}, "cell_type": "code", "source": "\nratings = (spark_df\n    .select(\n        'ID_CTE',\n        'ID_CLAS1',\n        'FREQUENCY',\n    )\n).cache() #  lo cargamos a memoria ram ", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Make sure your data is 'integer' type "}, {"metadata": {}, "cell_type": "markdown", "source": "### Spliting the data set to test and train for measuring the performance of the ALS Model"}, {"metadata": {}, "cell_type": "code", "source": "(training, test) = ratings.randomSplit([0.8, 0.2]) #  debemos validar esto ", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Build the recommendation model using ALS on the training data\n### Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. spark.ml currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.ml uses the alternating least squares (ALS) algorithm to learn these latent factors. "}, {"metadata": {}, "cell_type": "markdown", "source": "### Parameters of ALS Model in PySpark realization are following:\n\n##### NumBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation.\n##### rank is the number of latent factors in the model.\n##### maxIter is the maximum number of iterations to run.\n##### regParam specifies the regularization parameter in ALS.\n##### implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n##### alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0)"}, {"metadata": {}, "cell_type": "markdown", "source": "### Explicit vs. implicit feedback\n#### The standard approach to matrix factorization based collaborative filtering treats the entries in the user-item matrix as explicit preferences given by the user to the item, for example, users giving ratings to products.\n#### It is common in many real-world use cases to only have access to implicit feedback (e.g. views, clicks, purchases, likes, shares etc.). The approach used in spark.ml to deal with such data is taken from Collaborative Filtering for Implicit Feedback Datasets. Essentially, instead of trying to model the matrix of ratings directly, this approach treats the data as numbers representing the strength in observations of user actions (such as the number of clicks). Those numbers are then related to the level of confidence in observed user preferences, rather than explicit ratings given to items. The model then tries to find latent factors that can be used to predict the expected preference of a user for an item."}, {"metadata": {}, "cell_type": "code", "source": "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nt2 = time.time()\nals = ALS(maxIter=10, regParam=0.01, rank = 100, \n          userCol=\"ID_CTE\", itemCol=\"ID_CLAS1\", ratingCol=\"FREQUENCY\",\n          coldStartStrategy=\"drop\",\n          implicitPrefs=True)\n\nmodel = als.fit(ratings)\n\n# Evaluate the model by computing the RMSE on the test data\npredictions = model.transform(test)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"FREQUENCY\",\n                                predictionCol=\"prediction\")\nt3 = time.time()\nrmse = evaluator.evaluate(predictions)\nprint(\"Root-mean-square error = \" + str(rmse))\nprint('Tiempo de entranar y predecir    ' + str( (t3- t2)/60) + '    minutos') \n\n# rbarran con 300mil registros Tiempo de entranar y predecir    2.1862693349520366    minuto\n# rbarran con 3millones Tiempo de entranar y predecir    9.564896901448568    minutos\n# rbarran con 10millones Tiempo de entranar y predecir    14.262361097335816    minutos\n#rbarran con 30millones Tiempo de entranar y predecir    21.207575702667235    minutos", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "Root-mean-square error = 1.2610886477567078\nTiempo de entranar y predecir    39.46960059007009    minutos\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "###  Generate top k Item recommendations for each user\n#### The value of 'k' here is 10 and can be changed by passing the desired value to the function\n\n"}, {"metadata": {}, "cell_type": "code", "source": "userRecs = model.recommendForAllUsers(10)\n#userRecs.count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Display the results : Each row represents the 'k' recommendations for each User"}, {"metadata": {}, "cell_type": "code", "source": "#userRecs.take(10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### For getting each recommendation as a row in the final csv, we break down the result generated above using the explode function"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql.functions import explode\nuserRecs1=userRecs.withColumn(\"recommendations\", explode(userRecs.recommendations))\nuserRecs1.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "####  Breaking down reach recommendation to separate columns"}, {"metadata": {}, "cell_type": "code", "source": "#import select as s   PDA_Analytics\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "userRecs1= userRecs1 \\\n  .select('ID_CTE', 'recommendations.*')       ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Writing the Output back to the Remote Datasource\n#### Thereby the output or resulting csv can be consumed directly by anyone who has the access to the remote database "}, {"metadata": {}, "cell_type": "code", "source": "'''new_table_name = 'ANALITICAAFORE.ADMIN.RECOMMENDATIONSRESULT_TEST_CENIC'\n\nuserRecs1.coalesce(2).write.format('jdbc') \\\n    .mode('overwrite') \\\n    .option('url', 'jdbc:netezza://{}:{}/{}'.format(PDA_COPPEL_2020_credentials['host'],PDA_COPPEL_2020_credentials['port'], PDA_COPPEL_2020_credentials['database'])) \\\n    .option('dbtable', new_table_name) \\\n    .option('user', PDA_COPPEL_2020_credentials['username']).option('driver','org.netezza.Driver').option('password', PDA_COPPEL_2020_credentials['password']).save()\n'''    ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "userRecs1.show(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "new_table_name = 'RecommendationsResult_test_cenic'\n\nuserRecs1.coalesce(1).write.format('jdbc') \\\n    .mode('overwrite') \\\n    .option('url', 'jdbc:netezza://{}:{}/{}'.format(PDA_COPPEL_2020_credentials['host'],PDA_COPPEL_2020_credentials['port'],'CENIC')) \\\n    .option('dbtable', new_table_name) \\\n    .option('user', PDA_COPPEL_2020_credentials['username']).option('driver','org.netezza.Driver').option('password', PDA_COPPEL_2020_credentials['password']).save()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Saving the model for deployment in WML"}, {"metadata": {}, "cell_type": "code", "source": "#!pip install  --proxy=https://10.33.128.80:8080 dsx proxy para usar pip ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Pack the model inside a pipeline \n#### Since the WML deployments allow saving Spark Pipelines directly, put the ALS model inside a Pipeline for direct deployment stage\n#### Typically, A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. For this case, since the pipeline is bought in to action just for the sole cause of deployment, we do not use any transformers and such"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml import Pipeline", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pipeline = Pipeline(stages=[model])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_alsWML = pipeline.fit(ratings)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# model_alsWML.save('/temp/')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "t_final = time.time()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(t_final- t0)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print((t_final- t0) / 60 )", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 2}