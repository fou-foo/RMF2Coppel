{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q tensorflow-recommenders\n",
    "!pip install -q --upgrade tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import vstack\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import pdb;\n",
    "\n",
    "from subprocess import check_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm.evaluation import auc_score\n",
    "\n",
    "# Importing Libraries and cookbooks\n",
    "from recsys import *## recommender system cookbook\n",
    "#from generic_preprocessing import * ## pre-processing code\n",
    "from IPython.display import HTML ## Setting display options for Ipython Notebook\n",
    "from itertools import combinations, chain, product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.read_csv(\"demo001.csv\",sep=\"|\")\n",
    "\n",
    "demo.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[\"Fecha_Nac\"]=pd.to_datetime(demo[\"FECHA_NAC\"], format='%Y-%m-%d %H:%M')\n",
    "demo[\"Fecha_Nac\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoy=datetime.now().year\n",
    "def age(fecnan):\n",
    "    return (hoy-fecnan.year)\n",
    "\n",
    "demo[\"Edad\"]=demo[\"Fecha_Nac\"].apply(age)\n",
    "demo[\"Edad\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[\"edad\"]=pd.cut(demo['Edad'], [0,25,35,45,55,65,90]).astype(str)\n",
    "demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[\"NDepen\"]=demo['N_D_EC'].apply(lambda x: 0 if x == 0 else 1).astype(str)\n",
    "demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo=demo.loc[:,('ID_CTE', \"GENERO\", 'EDO_CIVIL',\"edad\",\"NDepen\",\"TP_HOGAR\")]\n",
    "df_demo[\"user_id\"]=df_demo[\"ID_CTE\"].astype(str)\n",
    "df_demo.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran = pd.read_csv(\"transactional001.csv\",sep=\"|\")\n",
    "\n",
    "tran.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tran=tran[((tran[\"PROD_AREA\"]==\"Muebles\")  | (tran[\"PROD_AREA\"]==\"Ropa\"))&((tran[\"IMPORTE_VTA\"]>100))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tran[\"area\"]=df_tran[\"PROD_AREA\"].apply(lambda x: 1 if x == 'Ropa' else 2) \n",
    "df_tran[\"fam\"]=df_tran[\"area\"].astype(str)+df_tran[\"PROD_DEP\"].astype(str)+df_tran[\"PROD_CLAS\"].astype(str).str.zfill(2)+df_tran[\"PROD_FAM\"].astype(str).str.zfill(3)\n",
    "df_tran[\"clas\"]=df_tran[\"area\"].astype(str)+df_tran[\"PROD_DEP\"].astype(str)+df_tran[\"PROD_CLAS\"].astype(str).str.zfill(2)\n",
    "df_tran[\"idclas\"]=df_tran[\"clas\"]\n",
    "df_tran[\"idfam\"]=df_tran[\"fam\"]\n",
    "df_tran[\"movie_id\"]=df_tran[\"fam\"].astype(str)#df_tran[\"DESC_PROD_DEP\"]#\n",
    "df_tran[\"user_id\"]=df_tran[\"ID_CTE\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f=df_tran.loc[:,('user_id', 'FECHA_TICKET', 'movie_id')].groupby(['user_id',\"movie_id\"]).size().reset_index()\n",
    "df_f.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1=df_f[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff1=pd.merge(df_f1, df_demo, on='user_id', how='inner')\n",
    "\n",
    "dff1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=dff1.dropna() \n",
    "df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=pd.DataFrame(df_1[\"movie_id\"].unique())\n",
    "dff[\"movie_id\"]=dff[0].astype(str)\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = pd.read_csv(\"prod_variables.csv\",sep=\",\")\n",
    "\n",
    "prod.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod[\"movie_id\"]=prod[\"id_Familia\"].astype(str)\n",
    "prod[\"id_Familia\"]=prod[\"id_Familia\"].astype(str)\n",
    "prod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfff=df_1.loc[:,(\"user_id\",\"movie_id\",\"GENERO\",\"EDO_CIVIL\",\"edad\",\"NDepen\",\"TP_HOGAR\")]\n",
    "dfff.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp=pd.merge(dfff, prod, on='movie_id', how='inner')\n",
    "\n",
    "dfp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpp=dff[\"movie_id\"].unique()#pd.DataFrame({\"movie_id\" : dff.loc[:,(\"movie_id\")]})#dff[\"movie_id\"].unique()#dfp.loc[:,\"movie_id\"]\n",
    "dfpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfff=df_1.loc[:,(\"user_id\",\"movie_id\",\"GENERO\",\"EDO_CIVIL\",\"edad\",\"NDepen\",\"TP_HOGAR\")]\n",
    "dfff.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp=pd.merge(prod,dfff, on='movie_id', how='inner')#df_1.loc[:,(\"user_id\",\"movie_id\",\"GENERO\",\"EDO_CIVIL\",\"edad\",\"NDepen\",\"TP_HOGAR\")]\n",
    "dfp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp=pd.merge(prod,dfff, on='movie_id', how='inner')#df_1.loc[:,(\"user_id\",\"movie_id\",\"GENERO\",\"EDO_CIVIL\",\"edad\",\"NDepen\",\"TP_HOGAR\")]\n",
    "dfp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmo=pd.DataFrame({\"movie_id\" : dff.loc[:,(\"movie_id\")]})#dff[\"movie_id\"].unique()#dfp.loc[:,\"movie_id\"]\n",
    "dfmovie=pd.merge(prod,dfmo, on='movie_id', how='inner')\n",
    "dfmovie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tf.data.Dataset.from_tensor_slices((dfp.to_dict('list')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = tfds.as_dataframe(ratings.take(10))\n",
    "dfr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies= tf.data.Dataset.from_tensor_slices((dfmovie.to_dict('list')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = tfds.as_dataframe(movies.take(10))\n",
    "dfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
    "#y=ratings.map(lambda x: x[\"user_id\"])\n",
    "\n",
    "movie_titles_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "movie_titles_vocabulary.adapt(movies.map(lambda x: x[\"movie_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestamps = np.concatenate(list(ratings.map(lambda x: x[\"GENERO\"].astype(int)).batch(50)))\n",
    "\n",
    "#max_timestamp = timestamps.max()\n",
    "#min_timestamp = timestamps.min()\n",
    "\n",
    "#timestamp_buckets = np.linspace(\n",
    " #   min_timestamp, max_timestamp, num=100,\n",
    "#)\n",
    "\n",
    "timestamps= np.unique(np.concatenate(list(ratings.batch(5_0).map(\n",
    "    lambda x: x[\"GENERO\"]))))\n",
    "\n",
    "EC= np.unique(np.concatenate(list(ratings.batch(5_0).map(\n",
    "    lambda x: x[\"EDO_CIVIL\"]))))\n",
    "\n",
    "ED= np.unique(np.concatenate(list(ratings.batch(5_0).map(\n",
    "    lambda x: x[\"edad\"]))))\n",
    "\n",
    "DE= np.unique(np.concatenate(list(ratings.batch(5_0).map(\n",
    "    lambda x: x[\"NDepen\"]))))\n",
    "\n",
    "\n",
    "TP= np.unique(np.concatenate(list(ratings.batch(5_0).map(\n",
    "    lambda x: x[\"TP_HOGAR\"]))))\n",
    "\n",
    "cat= np.unique(np.concatenate(list(ratings.batch(5_0).map(\n",
    "    lambda x: x[\"Categoria\"]))))\n",
    "\n",
    "fam= np.unique(np.concatenate(list(ratings.batch(5_0).map(\n",
    "    lambda x: x[\"Familia\"]))))\n",
    "\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(ratings.batch(5_0).map(\n",
    "    lambda x: x[\"movie_id\"]))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(5_0).map(\n",
    "    lambda x: x[\"user_id\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "  \n",
    "  def __init__(self, use_timestamps):\n",
    "    super().__init__()\n",
    "\n",
    "    self._use_timestamps = use_timestamps\n",
    "\n",
    "    self.user_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "            vocabulary=unique_user_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "    ])\n",
    "\n",
    "    if use_timestamps:\n",
    "      self.timestamp_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary=timestamps, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(timestamps) + 1, 32)\n",
    "        ])\n",
    "    \n",
    "      self.EC_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary=EC, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(EC) + 1, 32)\n",
    "        ])\n",
    "        \n",
    "      self.ED_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary=ED, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(ED) + 1, 32)\n",
    "        ])\n",
    "    \n",
    "      self.DE_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary=DE, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(DE) + 1, 32)\n",
    "        ])\n",
    "        \n",
    "      self.TP_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary=TP, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(TP) + 1, 32)\n",
    "       ])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    if not self._use_timestamps:\n",
    "      return self.user_embedding(inputs[\"user_id\"])\n",
    "    #tf.print(inputs[\"user_id\"])\n",
    "    return tf.concat([\n",
    "            self.user_embedding(inputs[\"user_id\"]),\n",
    "            self.timestamp_embedding(inputs[\"GENERO\"]),\n",
    "            self.EC_embedding(inputs[\"EDO_CIVIL\"]),\n",
    "            self.ED_embedding(inputs[\"edad\"]),\n",
    "            self.DE_embedding(inputs[\"NDepen\"]),\n",
    "            self.TP_embedding(inputs[\"TP_HOGAR\"])\n",
    "            ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "  \n",
    "  def __init__(self,use_timestamps):\n",
    "    super().__init__()\n",
    "    self._use_timestamps = use_timestamps\n",
    "    max_tokens = 10_000\n",
    "\n",
    "    self.title_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "          vocabulary=unique_movie_titles, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)\n",
    "    ])\n",
    "    \n",
    "    self.title_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=max_tokens)\n",
    "\n",
    "    self.title_text_embedding = tf.keras.Sequential([\n",
    "      self.title_vectorizer,\n",
    "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "      tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    ])\n",
    "\n",
    "    #self.title_vectorizer.adapt(movies)\n",
    "    self.title_vectorizer.adapt(ratings.map(lambda x: x[\"movie_id\"]))\n",
    "    if use_timestamps:\n",
    "        self.cat_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary=cat, mask_token=None),\n",
    "        tf.keras.layers.Embedding(len(cat) + 1, 32)\n",
    "        ])\n",
    "        \n",
    "        self.fam_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary=fam, mask_token=None),\n",
    "        tf.keras.layers.Embedding(len(fam) + 1, 32)\n",
    "        ])\n",
    "    \n",
    "   # def call(self, inputs):\n",
    "    #if not self._use_timestamps:\n",
    "     # return self.title_embedding(titles[\"movie_id\"]),\n",
    "      #  self.title_text_embedding(titles[\"movie_id\"])\n",
    "    \n",
    "  def call(self, titles):\n",
    "    if not self._use_timestamps:\n",
    "      return tf.concat([\n",
    "            self.title_embedding(titles[\"movie_id\"]),\n",
    "            self.title_text_embedding(titles[\"movie_id\"]),\n",
    "        ], axis=1)\n",
    "\n",
    "      #tf.print(titles[\"movie_id\"])\n",
    "    return tf.concat([\n",
    "            self.title_embedding(titles[\"movie_id\"]),\n",
    "            self.title_text_embedding(titles[\"movie_id\"]),\n",
    "            self.cat_embedding(titles[\"Categoria\"]),\n",
    "            self.fam_embedding(titles[\"Familia\"])\n",
    "            ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self, use_timestamps):\n",
    "    super().__init__()\n",
    "    self.query_model = tf.keras.Sequential([\n",
    "      UserModel(use_timestamps),\n",
    "      tf.keras.layers.Dense(32)\n",
    "    ])\n",
    "    self.candidate_model = tf.keras.Sequential([\n",
    "      MovieModel(use_timestamps),\n",
    "      tf.keras.layers.Dense(32)\n",
    "    ])\n",
    "    self.task = tfrs.tasks.Retrieval(\n",
    "        metrics=tfrs.metrics.FactorizedTopK(\n",
    "            candidates=movies.batch(128).map(self.candidate_model),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    # We only pass the user id and timestamp features into the query model. This\n",
    "    # is to ensure that the training inputs would have the same keys as the\n",
    "    # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "    # error when loading the query model after saving it.\n",
    "    query_embeddings = self.query_model({\n",
    "        \"user_id\": features[\"user_id\"],\n",
    "        \"GENERO\": features[\"GENERO\"],\n",
    "        \"EDO_CIVIL\": features[\"EDO_CIVIL\"],\n",
    "        \"edad\": features[\"edad\"],\n",
    "         \"NDepen\": features[\"NDepen\"],\n",
    "         \"TP_HOGAR\": features[\"TP_HOGAR\"]\n",
    "                                            })\n",
    "    #movie_embeddings = self.candidate_model(features[\"movie_id\"])\n",
    "    \n",
    "    movie_embeddings = self.candidate_model({\n",
    "       \"movie_id\": features[\"movie_id\"],\n",
    "       \"Categoria\": features[\"Categoria\"],\n",
    "        \"Familia\": features[\"Familia\"]\n",
    "                                                 \n",
    "                                                 })\n",
    "    \n",
    "    #movie_embeddings = self.candidate_model(features[\"movie_title\"])\n",
    "\n",
    "    return self.task(query_embeddings, movie_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(1_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(8_000)\n",
    "test = shuffled.skip(8_000).take(2_000)\n",
    "\n",
    "cached_train = train.shuffle(1_000).batch(204)\n",
    "cached_test = test.batch(406).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel(use_timestamps=False)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "model.fit(cached_train, epochs=3)\n",
    "\n",
    "train_accuracy = model.evaluate(\n",
    "    cached_train, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "test_accuracy = model.evaluate(\n",
    "    cached_test, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "\n",
    "print(f\"Top-100 accuracy (train): {train_accuracy:.2f}.\")\n",
    "print(f\"Top-100 accuracy (test): {test_accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel(use_timestamps=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "model.fit(cached_train, epochs=3)\n",
    "\n",
    "train_accuracy = model.evaluate(\n",
    "    cached_train, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "test_accuracy = model.evaluate(\n",
    "    cached_test, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "\n",
    "print(f\"accuracy (train): {train_accuracy:.2f}.\")\n",
    "print(f\"accuracy (test): {test_accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
